# @package _global_
general:
  project: 'retrodiffuser' # for wandb logging
  name : 'mol-4layer-posenc'
  task: 'mol'
diffusion: 
  transition: 'absorbing_masknoedge'
  denoiser: 'neuralnet'
  edge_conditional_set: 'test'
  diffuse_edges: False
  diffusion_noise_schedule: 'cosine'    
  diffusion_steps: 1
  lambda_train: [1, 0]
  ce_lambda: 0.01 # used with loss vb+ce
  extra_features: null
  mask_nodes: null # sn, reactant, product_and_sn, reactant_and_sn, first_n, ring
  mask_edges: null
  node_states_to_mask: ['none']
  edge_states_to_mask: ['none']
neuralnet:
  use_ema: False
  ema_decay: 0.999
  checkpoint_file: null
  architecture: 'with_y_pos_enc'
  n_layers: 4
  hidden_mlp_dims: {'X': 256, 'E': 128, 'y': 128}
  hidden_dims : {'dx': 256, 'de': 64, 'dy': 64, 'n_head': 8, 'dim_ffX': 256, 
                 'dim_ffE': 128, 'dim_ffy': 128}
  # hidden_dims : {'dx': 32, 'de': 1, 'dy': 1, 'n_head': 1, 'dim_ffX': 16, 'dim_ffE': 1, 'dim_ffy': 1}
  # hidden_mlp_dims: {'X': 16, 'E': 1,'y': 1}
dataset:
  name: uspto-50k
  dataset_nb: ''
  num_workers: 0
  shuffle: True
train:
  with_mask: False
  seed: 1
  loss: 'ce'
  epochs: 2000
  lr: 1e-3
  batch_size: 512
  batch_by_size: False
  log_to_wandb: True
  log_grad: False
  grad_every_step: 100
  eval_every_epoch: 200
  log_every_t: 1
  chains_to_save: 2
  samples_to_generate: 2
test:
  batch_size: 100
  smiles_accuracy: True
  size_test_splits: 100
  with_denoising: True
  n_samples: 1000
  repeat_sampling: 10 # mainly used for conditional generation where we need x samples per test case => loop for memory 
  wandb_log_samples: True
