# @package _global_
dataset:
  add_supernode_edges: false
  batchsize_bins:
    test:
    - 64
    train:
    - 120
    - 56
    - 16
    - 8
    val:
    - 64
  canonicalize_molecule: true
  datadir: data/uspto-50k
  datadist_dir: data/uspto-50k
  dataset_nb: dummy15-nocharge
  with_formal_charge: False
  name: uspto-50k
  atom_types: ['none', 'O', 'C', 'N', 'I', 'Cl', 'Si', 'F', 'Br', 'S', 'B', 'Cu', 'Sn', 'P', 'Se', 'Zn', 'Mg', 'Au', 'U', 'SuNo']
  allowed_bonds:
    'O': [2]
    'C': [4]
    'N': [3]
    'I': [1, 3, 5, 7]
    'Cl': [1]
    'Si': [4, 6]
    'F': [1]
    'Br': [1]
    'N+1': [4]
    'O-1': [1]
    'S': [2, 4, 6]
    'B': [3]
    'N-1': [2]
    'Zn+1': [3]
    'Cu': [1, 2] 
    'Sn': [2, 4]
    'P+1': [4, 6, 8]
    'Mg+1': [3]
    'C-1': [3]
    'P': [3, 5, 7]
    'S+1': [3, 5, 7]
    'S-1': [1, 3, 5]
    'Se': [2, 4, 6]
    'Zn': [2]
    'Mg': [2]
    'U': [6]
    'Au': [0]
  nb_rct_dummy_nodes: 15
  num_workers: 0
  pin_memory: false
  remove_h: false
  shuffle: true
  size_bins:
    test:
    - 250
    train:
    - 64
    - 83
    - 102
    - 140
    val:
    - 250
  with_explicit_h: false
diffusion:
  ce_lambda: 0.01
  classifier_free_drop_within_batch: false
  classifier_free_full_unconditioning: false
  classifier_free_guidance: false
  classifier_free_guidance_weight: 0.1
  classifier_free_uncond_prob: 0.1
  denoiser: neuralnet
  diffuse_edges: true
  diffuse_nodes: true
  diffusion_noise_schedule: cosine
  diffusion_steps_eval: 100
  diffusion_steps: 100
  edge_conditional_set: val
  edge_states_to_mask:
  - mol
  - within
  - across
  extra_features: null
  lambda_test: 1
  lambda_train:
  - 5
  - 0
  mask_edges: product_and_sn
  mask_nodes: product_and_sn
  node_states_to_mask:
  - SuNo
  - none
  num_node_steps: 1
  temperature_scaling_edge: 1
  temperature_scaling_node: 1
  transition: absorbing_masknoedge
experiment_group: if3aizpe
general:
  gpus: 1
  log_every_steps: 50
  name: rxn_absorbing_masknoedge_product_and_sn_dummy15_loss_ce_laplacian_pos_enc_no_clfgnum_gpus_2_simplebatching_16_lr_0.0002_lamtrain_5_total_epochs_700_num_ev_20-seed1
  project: retrodiffuser
  resume_from_last_n: 1
  task: rxn
  wandb:
    entity: najwalb
    mode: online
    project: retrodiffuser
    resume: true
    run_id: if3aizpe
  wandb_id: cht3b5u4
neuralnet:
  architecture: with_y_atommap_number_pos_enc
  atom_map_pos_encoding: false
  checkpoint_file: null
  dropout: 0.1
  ema_decay: 0.9
  extra_features: true
  hidden_dims:
    de: 64
    dim_ffE: 128
    dim_ffX: 256
    dim_ffy: 128
    dx: 256
    dy: 64
    n_head: 8
  hidden_mlp_dims:
    E: 128
    X: 256
    y: 128
  improved: false
  increase_y_dim_for_multigpu: true
  load_checkpoint: false
  n_layers: 9
  num_lap_eig_vectors: 20
  pos_emb_permutations: -1
  pos_encoding_type: laplacian_pos_enc
  use_all_gpus: true
  use_ema: false
test:
  batch_size: 32
  chains_to_save: 1
  elbo_samples: 32
  eval_before_first_epoch: false
  full_dataset: false
  loss_0_repeat: 100
  n_conditions: 8
  n_samples: 1
  n_samples_per_condition: 100
  plot_dummy_nodes: false
  plot_rxn_chains: true
  rep_samples_per_condition: 1
  repeat_elbo: 1
  size_test_splits: 100
  smiles_accuracy: true
  sort_lambda_value: 0.9
  testfile: 0
  topks:
  - 1
  - 3
  - 5
  - 10
  - 50
  - 100
  wandb_log_samples: true
  wandb_run_config: 2609_marginal_reactant_and_sn_dummy0_ce_clfgTrue_config.pickle
  wandb_run_name: rxn_marginal_reactant_and_sn_dummy0_ce_clfgTrue
  with_denoising: true
train:
  amsgrad: true
  batch_by_size: false
  batch_size: 16
  best_model_criterion: top-5
  chains_to_save: 4
  clip_grad: null
  ema_decay: 0
  epochs: 700
  eval_every_epoch: 20
  final_lr: 4.0e-05
  grad_every_step: 100
  initial_lr: 1.0e-07
  log_every_t: 50
  log_grad: false
  log_to_wandb: true
  loss: ce
  lr: 0.0002
  lr_scheduler: none
  num_annealing_epochs: 700
  num_warmup_epochs: 0
  num_workers: 0
  optimizer: adamw
  overfit: falsesac
  progress_bar: false
  samples_to_generate: 4
  save_every_epoch: false
  save_model: true
  save_models_at_all: true
  seed: 1
  weight_decay: 1.0e-12
  with_mask: false
