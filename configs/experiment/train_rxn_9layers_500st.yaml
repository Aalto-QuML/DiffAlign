# @package _global_
general:
  project: 'retrodiffuser' # for wandb logging
  name : 'rxn-edgegen-9layer-200ep-100st-seed'
  task: 'rxn'
diffusion: 
  transition: 'absorbing_masknoedge'
  denoiser: 'neuralnet'
  edge_conditional_set: 'val'
  diffuse_edges: True
  diffusion_noise_schedule: 'cosine'    
  diffusion_steps: 500
  lambda_train: [5, 0]
  ce_lambda: 0.01 # used with loss vb+ce
  mask_nodes: 'product_and_sn' # sn, reactant, product_and_sn, reactant_and_sn, first_n, ring
  mask_edges: 'product_and_sn'
  node_states_to_mask: ['SuNo', 'none']
  edge_states_to_mask: ['mol', 'within', 'across']
neuralnet:
  use_ema: False
  ema_decay: 0.999
  checkpoint_file: null
  architecture: 'with_y'
  n_layers: 9
  hidden_mlp_dims: {'X': 256, 'E': 128, 'y': 128}
  hidden_dims : {'dx': 256, 'de': 64, 'dy': 64, 'n_head': 8, 'dim_ffX': 256, 'dim_ffE': 128, 'dim_ffy': 128}
  # hidden_dims : {'dx': 32, 'de': 1, 'dy': 1, 'n_head': 1, 'dim_ffX': 16, 'dim_ffE': 1, 'dim_ffy': 1}
  # hidden_mlp_dims: {'X': 16, 'E': 1,'y': 1}
  extra_features: True
dataset:
  name: uspto-50k
  dataset_nb: ''
  num_workers: 0
  shuffle: True
train:
  with_mask: False
  seed: 1
  loss: 'ce'
  epochs: 500
  lr: 1e-4
  batch_size: 16
  batch_by_size: True
  log_to_wandb: True
  log_grad: False
  grad_every_step: 100
  eval_every_epoch: 40
  log_every_t: 20
  chains_to_save: 4
  samples_to_generate: 4
  best_model_criterion: 'top-5'
test:
  testfile: 0
  full_dataset: False
  elbo_samples: 16
  n_conditions: 8 # total number of samples from the test set
  batch_size: 16 # size of batch to iterate over 
  n_samples_per_condition: 100
  rep_samples_per_condition: 1 # max samples to generate for each batch (for memory reasons)
  smiles_accuracy: True
  size_test_splits: 100
  with_denoising: True
  topks: [1, 3, 5, 10, 50, 100]
  wandb_log_samples: False
  loss_0_repeat: 100