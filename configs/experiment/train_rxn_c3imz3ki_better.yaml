# @package _global_
dataset:
  name: uspto-50k
  datadir: data/uspto-50k
  shuffle: true
  remove_h: false
  size_bins:
    val:
    - 250
    test:
    - 250
    train:
    - 64
    - 83
    - 102
    - 140
  dataset_nb: dummy15
  pin_memory: false
  num_workers: 0
  datadist_dir: data/uspto-50k
  batchsize_bins:
    val:
    - 64
    test:
    - 64
    train:
    - 120
    - 56
    - 16
    - 8
  with_explicit_h: false
  nb_rct_dummy_nodes: 15
  with_formal_charge: true
  add_supernode_edges: true
  canonicalize_molecule: true
diffusion:
  denoiser: neuralnet
  ce_lambda: 0.01
  mask_edges: product_and_sn
  mask_nodes: product_and_sn
  lambda_test: 1
  lambda_train:
  - 5
  - 0
  diffuse_edges: true
  diffuse_nodes: true
  extra_features: null
  num_node_steps: 1
  edge_states_to_mask:
  - mol
  - within
  - across
  node_states_to_mask:
  - SuNo
  - none
  edge_conditional_set: val
  classifier_free_guidance: true
  diffusion_noise_schedule: cosine
  temperature_scaling_edge: 1
  temperature_scaling_node: 1
  classifier_free_uncond_prob: 0.1
  classifier_free_guidance_weight: 0.1
  classifier_free_drop_within_batch: false
  classifier_free_full_unconditioning: false
  transition: marginal
  diffusion_steps: 500
general:
  gpus: 1
  log_every_steps: 50
  name: general
  project: retrodiffuser
  resume_from_last_n: 1
  task: rxn
  wandb: 
    entity: najwalb
    project: retrodiffuser
    mode: online
    resume: false
    run_name: null
    run_id: null # id of the training run to use (resume/sample from)
    group: sota
    tags: ["template_free", "ablations", "fine-tuning"]
    checkpoint_epochs: null # epoch num for the checkpoint to use (resume/sample from)
neuralnet:
  use_ema: false
  n_layers: 9
  ema_decay: 0.999
  hidden_dims:
    de: 64
    dx: 256
    dy: 64
    n_head: 8
    dim_ffE: 128
    dim_ffX: 256
    dim_ffy: 128
  architecture: with_y
  use_all_gpus: true
  extra_features: true
  checkpoint_file: null
  hidden_mlp_dims:
    E: 128
    X: 256
    y: 128
  load_checkpoint: false
  pos_encoding_type: smiles_pos_enc
  pos_emb_permutations: 0
  atom_map_pos_encoding: false
  increase_y_dim_for_multigpu: true
  dropout: 0.1
  improved: false
test:
  topks: [1, 3, 5, 10, 50, 100]
  testfile: 0
  n_samples: 1
  batch_size: 32
  repeat_elbo: 1
  elbo_samples: 32
  full_dataset: false
  n_conditions: 8
  loss_0_repeat: 100
  chains_to_save: 1
  with_denoising: true
  smiles_accuracy: true
  size_test_splits: 100
  wandb_log_samples: true
  eval_on_first_epoch: false
  n_samples_per_condition: 100
  rep_samples_per_condition: 1
  plot_dummy_nodes: false
train:
  loss: ce
  seed: 1
  epochs: 340
  amsgrad: true
  overfit: falsesac
  log_grad: false
  clip_grad: null
  ema_decay: 0
  optimizer: adamw
  with_mask: false
  batch_size: 16
  save_model: true
  log_every_t: 10
  num_workers: 0
  log_to_wandb: true
  lr_scheduler: none
  progress_bar: false
  weight_decay: 1.0e-12
  batch_by_size: false
  chains_to_save: 4
  grad_every_step: 100
  eval_every_epoch: 20
  save_every_epoch: false
  num_warmup_epochs: 2
  save_models_at_all: true
  samples_to_generate: 4
  best_model_criterion: top-5
  lr: 0.00025
  final_lr: 5.0e-05
  initial_lr: 1.0e-07
  num_annealing_epochs: 340 # if num_annealing_epochs=epochs => no annealing